import torch
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import accuracy_score
import pandas as pd
import re
import unicodedata
from typing import Union
import numpy as np
import os

def get_eval_model(model_path: str, device, unique_labels):
    model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(unique_labels)).to(device)
    model_fp16 = AutoModelForSequenceClassification.from_pretrained(model_path+"_fp16", num_labels=len(unique_labels)).to(device).half()

    model_fp32.eval()
    model_fp16.eval()

    return model_fp32, model_fp16

def normalize_unicode_text(text: str) -> str:
    normalized = unicodedata.normalize("NFKC", text)
    unicode_single_hangul_dict = {'á„€': 'ã„±', 'á„‚': 'ã„´', 'á„ƒ': 'ã„·', 'á„…': 'ã„¹', 'á„†': 'ã…', 'á„‡': 'ã…‚', 'á„‰': 'ã……', 'á„‹': 'ã…‡', 'á„Œ': 'ã…ˆ', 'á„': 'ã…Š', 'á„': 'ã…‹', 'á„': 'ã…Œ', 'á„‘': 'ã…', 'á„’': 'ã…', 'á„': 'ã…‰', 'á„„': 'ã„¸', 'á„': 'ã„²', 'á„Š': 'ã…†', 'á…¡': 'ã…', 'á…£': 'ã…‘', 'á…¥': 'ã…“', 'á…§': 'ã…•', 'á…©': 'ã…—', 'á…­': 'ã…›', 'á…®': 'ã…œ', 'á…²': 'ã… ', 'á…³': 'ã…¡', 'á…µ': 'ã…£', 'á…¢': 'ã…', 'á…¦': 'ã…”', 'á…´': 'ã…¢', 'á†ª': 'ã„±ã……', 'á†¬': 'ã„´ã…ˆ', 'á†­': 'ã„´ã…', 'á†²': 'ã„¹ã…‚', 'á†°': 'ã„¹ã„±', 'á†³': 'ã„¹ã……', 'á†±': 'ã„¹ã…', 'á„š': 'ã„¹ã…', 'á†´': 'ã„¹ã…Œ', 'á†µ': 'ã„¹ã…', 'á„¡': 'ã…‚ã……', 'á„ˆ': 'ã…‚ã…‚'}
    normalized = ''.join(ch for ch in normalized if not unicodedata.combining(ch))
    return ''.join(unicode_single_hangul_dict[ch] if ch in unicode_single_hangul_dict else ch for ch in normalized)

def normalize_tlettak_font(text: str, 
                           space_pattern: Union[str, re.Pattern] = r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9]+[\s!?@.,â¤]*', 
                           search_pattern: Union[str, re.Pattern] = r'(\b\w\b)([\s!?@.,â¤]+)(\b\w\b)',
                           ) -> str:
    if isinstance(space_pattern, str):
        space_pattern = re.compile(space_pattern)
    if isinstance(search_pattern, str):
        search_pattern = re.compile(search_pattern)

    result = []
    sub = []
    pos = 0
    
    while pos < len(text):
        space_matched = space_pattern.match(text, pos)
        search_matched = search_pattern.match(text, pos)

        if search_matched:
            sub.extend([search_matched.group(1), search_matched.group(3)])
            pos = search_matched.end() - 1
        elif space_matched:
            s_end = space_matched.end()
            result.append(''.join(sub[::2]) + text[pos:s_end].strip())
            pos = s_end
            sub.clear()
        else:   # ë‘˜ ë‹¤ ë§¤ì¹­ ì‹¤íŒ¨ì¸ ê²½ìš° ë’·ë¬¸ì¥ ì „ë¶€ë¥¼ ë¶™ì—¬ì”€
            result.append(text[pos:])
            break
    return ' ' .join(result)

def replace_nickname_data(df: pd.DataFrame):
    space_pattern = r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9:]+[\s!?@.,â¤]*'
    pattern = r"(\w)([\s!?@.,â¤]+)(\b\w\b)"

    # prefix, subfix ì œê±°
    df['nickname'] = df['nickname']\
        .str.strip()\
        .str.replace('@', '')\
        .str.replace(r'-[a-zA-Z0-9]+(?=\s|$)', '', regex=True)
    # íŠ¹ìˆ˜ ê¸°í˜¸ ì œê±°
    df['nickname'] = df['nickname']\
        .str.replace(r'[-._]', '', regex=True)
    # ì˜ì–´, í•œê¸€, ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ë‹‰ë„¤ì„ ì²˜ë¦¬
    df['nickname'] = df['nickname']\
        .str.replace(r'[^a-zA-Zê°€-í£ã„±-ã…ã…-ã…£0-9]+', '[DEFAULT_NICK]', regex=True)
    
    with open('./tokens/emojis.txt', 'r', encoding='utf-8') as f:
        emojis = [line.strip() for line in f.readlines()]

    emoji_pattern = '|'.join(map(re.escape, emojis))
    df['comment'].str.replace(emoji_pattern, '[TEXT_EMOJI]', regex=True)
    
    # ìœ ë‹ˆì½”ë“œ ë¬¸ì¥ë¶€í˜¸ ìˆ˜ì •
    df['comment'] = df['comment']\
        .str.replace(r'[ã†Â·ãƒ»â€¢]', '.', regex=True)\
        .str.replace(r'[á†¢â€¦]+', '..', regex=True)\
        .str.replace(r'[â€˜â€™]+', "'", regex=True)\
        .str.replace(r'[â€œâ€]+', '"', regex=True)\
        .str.replace(r'[\u0020\u200b\u2002\u2003\u2007\u2008\u200c\u200d]+', ' ', regex=True)\
        .str.replace(r'[\U0001F3FB-\U0001F3FF\uFE0F]', '', regex=True)
    # ìœ ë‹ˆì½”ë“œ ê¾¸ë°ˆ ë¬¸ì(ê²°í•© ë¬¸ì) ì œê±°
    df['comment'] = df['comment'].str.replace(r'\*+', '', regex=True)
    df['comment'] = df['comment'].apply(lambda x: normalize_unicode_text(x) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'https?:\/\/(?:[a-zA-Z0-9-]+\.)*[a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£-]+\.[a-zA-Z]{2,}(?:\/[^?\s]*)?(?:\?[^\s]*)?', '[URL]', regex=True)\
        .str.replace(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[EMAIL]', regex=True)\
    # í•œê¸€ì + ë¶€í˜¸ + í•œê¸€ì íŒ¨í„´ ì²˜ë¦¬
    df['comment'] = df['comment'].apply(lambda x: normalize_tlettak_font(x, space_pattern, pattern) if isinstance(x, str) else x)
    # special token íŒŒì‹±
    df['comment'] = df['comment']\
        .str.replace(r'@{1,2}[A-Za-z0-9ê°€-í£\_\-\.]+', '[TAG]', regex=True)\
        .str.replace(r'#[A-Za-z0-9ã„±-ã…ã…-ã…£ê°€-í£\_\-\.]+', '[HASH_TAG]', regex=True)\
        .str.replace('Â¡', '!').str.replace('Â¿', '?')\
        .str.replace(r'([ğŸ‘‡âœ‹ğŸ‘])', '[THUMB]', regex=True)\
        .str.replace(r'([â¡â¬‡â†—â†˜â†–â†™â†’â†â†‘â†“â‡’]|[\-\=]+>|<[\-\=]+)', '[ARROW]', regex=True)\
        .str.replace(r'[ğŸ’šğŸ’›ğŸ©·ğŸ©¶ğŸ’—ğŸ’–â¤ğŸ©µğŸ–¤ğŸ’˜â™¡â™¥ğŸ§¡ğŸ”¥ğŸ’•ï¸ğŸ¤ğŸ’œğŸ¤ğŸ’™]', '[HEART]', regex=True)\
        .str.replace(r'ğŸ‰', '[CONGRAT]', regex=True)
    # ì“¸ë°ì—†ì´ ë§ì€ ë¬¸ì¥ë¶€í˜¸ ì œê±°
    df['comment'] = df['comment']\
        .str.replace(r'([^\s])[.,](?=\S)', r'\1', regex=True)\
        .str.replace(r'([.,?!^]+)', r' \1 ', regex=True)\
        .str.replace(r'\s+([.,?!^]+)', r'\1', regex=True)\
        .str.replace(r'\s{2,}', ' ', regex=True)
    # timestamp ì²˜ë¦¬
    to_replace = '[TIMESTAMP]'
    df['comment'] = df['comment']\
        .str.replace(r'\d+:(?:\d+:?)?\d+', to_replace, regex=True)
    # ë°ˆ ì²˜ë¦¬
    # df['comment'] = df['comment']\
    #     .str.replace(r'(?i)chill', 'ì¹ ', regex=True)
    # í•œê¸€, ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'[^a-zA-Z0-9ê°€-í£ã„±-ã…ã…-ã…£â™¡â™¥\!\?\@\#\$\%\^\&\*\(\)\-\_\=\+\\\~\,\.\/\<\>\[\]\{\}\;\:\'\"\s]', '', regex=True)
    # 2ê°œ ì´ìƒ ì—°ì†ëœ ë¬¸ì ì²˜ë¦¬
    df['comment'] = df['comment']\
        .str.replace(r'(.)\1{2,}', r'\1\1', regex=True)
    # ë¹ˆ ë¬¸ìì—´ì˜ ê²½ìš° empty ì²˜ë¦¬
    df['comment'] = df['comment'].str.strip()
    df['comment'] = df['comment'].fillna('[EMPTY]')

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.encodings = tokenizer(
            texts.tolist(), 
            truncation=True, 
            padding=True, 
            max_length=max_length, 
            return_tensors='pt'
        )
        self.labels = labels
    
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item
    
    def __len__(self):
        return len(self.labels)

def prepare_data(datas, labels, tokenizer, max_length, batch_size=32):
    """
    ë°ì´í„° ì¤€ë¹„ ë° ë°ì´í„°ë¡œë” ìƒì„±
    """
    # ë°ì´í„° ë¶„í• 
    dataset = CustomDataset(datas, labels, tokenizer, max_length)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    return test_loader

def test_fp_precision_eval(model_type, dataloader_fp32, dataloader_fp16, device, unique_labels):
    import time
    """
    FP32ì™€ FP16 ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    model_fp32, model_fp16 = get_eval_model(os.path.join(os.path.expanduser('~'), 'youtube-comment-colab', 'model', f'{model_type}_model'), device, unique_labels)
    
    fp32_correct = 0
    fp32_total = 0
    fp32_total_loss = 0.0
    fp32_time = 0
    
    fp16_correct = 0
    fp16_total = 0
    fp16_total_loss = 0.0
    fp16_time = 0

    start = 0
    
    fp32_start = time.perf_counter_ns()
    with torch.no_grad():
        for batch in dataloader_fp32:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            labels = batch['labels'].to(device)
            
            # FP32 ëª¨ë¸ í‰ê°€
            with torch.no_grad():
                # inputs_fp32 = {k: v.float() for k, v in inputs.items()}
                start = time.perf_counter_ns()
                outputs_fp32 = model_fp32(**inputs)
                # loss_fp32 = outputs_fp32.loss()
            
                predictions_fp32 = torch.argmax(outputs_fp32.logits, dim=-1)
                fp32_time += time.perf_counter_ns() - start
            fp32_total += labels.size(0)
            fp32_correct += (predictions_fp32 == labels).sum().item()
            # fp32_total_loss += loss_fp32.item()
    print('\t\tdataloader fp32 - ', (time.perf_counter_ns() - fp32_start) / 1e9)

    fp16_start = time.perf_counter_ns()
    with torch.no_grad():
        for batch in dataloader_fp16:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            labels = batch['labels'].to(device)
            with torch.no_grad():
                # FP16 ëª¨ë¸ í‰ê°€
                # inputs_fp16 = {k: v.half() for k, v in inputs.items()}
                start = time.perf_counter_ns()
                outputs_fp16 = model_fp16(**inputs)
                # loss_fp16 = outputs_fp16.loss()
            
                predictions_fp16 = torch.argmax(outputs_fp16.logits, dim=-1)
                fp16_time += time.perf_counter_ns() - start
            fp16_total += labels.size(0)
            fp16_correct += (predictions_fp16 == labels).sum().item()
            # fp16_total_loss += loss_fp16.item()
    print('\t\tdataloader fp16 - ', (time.perf_counter_ns() - fp16_start) / 1e9)
    
    # ì •í™•ë„ ë° ì†ì‹¤ ê³„ì‚°
    fp32_accuracy = fp32_correct / fp32_total
    fp16_accuracy = fp16_correct / fp16_total
    
    # fp32_avg_loss = fp32_total_loss / len(test_dataloader)
    # fp16_avg_loss = fp16_total_loss / len(test_dataloader)
    
    return {
        'fp32_accuracy': fp32_accuracy,
        'fp16_accuracy': fp16_accuracy,
        # 'fp32_avg_loss': fp32_avg_loss,
        # 'fp16_avg_loss': fp16_avg_loss,
        'fp32_correct': fp32_correct,
        'fp16_correct': fp16_correct,
        'fp32_total': fp32_total,
        'fp16_total': fp16_total,
        'fp32_time': fp32_time,
        'fp16_time': fp16_time,
    }

def print_fp_precision_evaluation(eval_results, model_type: str):
    """
    FP32ì™€ FP16 ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì¶œë ¥
    """
    print(f"{model_type} ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
    
    print(f"FP32 ì‹¤í–‰ ì‹œê°„: {eval_results['fp32_time'] / 1e9}")
    print(f"FP16 ì‹¤í–‰ ì‹œê°„: {eval_results['fp16_time'] / 1e9}")

    print(f"\nFP32 ì •í™•ë„: {eval_results['fp32_accuracy'] * 100:.2f}%")
    print(f"FP16 ì •í™•ë„: {eval_results['fp16_accuracy'] * 100:.2f}%")
    print(f"ì •í™•ë„ ì°¨ì´: {abs(eval_results['fp32_accuracy'] - eval_results['fp16_accuracy']) * 100:.2f}%")
    
    # print(f"\nFP32 í‰ê·  ì†ì‹¤: {eval_results['fp32_avg_loss']:.4f}")
    # print(f"FP16 í‰ê·  ì†ì‹¤: {eval_results['fp16_avg_loss']:.4f}")
    
    print(f"\nFP32 ì •í™•í•œ ì˜ˆì¸¡: {eval_results['fp32_correct']} / {eval_results['fp32_total']}")
    print(f"FP16 ì •í™•í•œ ì˜ˆì¸¡: {eval_results['fp16_correct']} / {eval_results['fp16_total']}")

def eval(df: pd.DataFrame, model_type: str, tokenizer):
    import time
    print(f'==================== {model_type} ëª¨ë¸ í‰ê°€ ë¡œê·¸ ====================')
    start = time.perf_counter_ns()
    columns = df[[f'{model_type}', f'{model_type}_class']].dropna().reset_index(drop=True)
    label_map = {label: idx for idx, label in enumerate(columns[f'{model_type}_class'].unique())}
    labels = columns[f'{model_type}_class'].map(label_map)

    max_token_length = 40 if model_type == 'nickname' else 256

    datas = columns[model_type]

    batch_size = 16

    pd_start = time.perf_counter_ns()
    test_loader_fp32 = prepare_data(
        datas,
        labels,
        tokenizer,
        max_token_length,
        batch_size
    )
    print('\tprepare fp32 data time: ', (time.perf_counter_ns() - pd_start) / 1e9)

    pd_start = time.perf_counter_ns()
    test_loader_fp16 = prepare_data(
        datas,
        labels,
        tokenizer,
        max_token_length,
        batch_size * 2
    )
    print('\tprepare fp16 data time: ', (time.perf_counter_ns() - pd_start) / 1e9)

    eval_start = time.perf_counter_ns()
    eval_results = test_fp_precision_eval(
        model_type=model_type,
        dataloader_fp32=test_loader_fp32, 
        dataloader_fp16=test_loader_fp16, 
        device=device,
        unique_labels=columns[f'{model_type}_class'].unique()
    )
    print('\ttotal eval time: ', (time.perf_counter_ns() - eval_start) / 1e9)

    print_fp_precision_evaluation(eval_results, model_type)
    print('ì „ì²´ ì¶”ë¡  ì‹œê°„: ', (time.perf_counter_ns() - start) / 1e9)

from s3_helper import S3Helper
helper = S3Helper('/home/sh/youtube-comment-colab', 'youtube-comment-predict')
print('download dataset.csv...')
helper.download(['dataset.csv'])

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = AutoTokenizer.from_pretrained("./model/tokenizer")

df = pd.read_csv(("./model/dataset.csv"), usecols=["nickname", "comment", "nickname_class", "comment_class"])
df['comment'] = df['comment'].str.replace(r'\\', ',', regex=True) 
replace_nickname_data(df)

eval(df, 'nickname', tokenizer)
eval(df, 'comment', tokenizer)